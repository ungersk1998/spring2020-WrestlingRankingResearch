{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from random import uniform\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRESTLING_URL = 'https://www.trackwrestling.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aiming to place scraped data in its own folder\n",
    "OUTPUT_FILE = './ScrapedMatchData/match_info_{}.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notes:\n",
    "# Events page caps at 250 events, which is roughly the last week. Can't keep scrolling like the indeed page number code\n",
    "# Maybe need to send specific search params by date?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remote browsing through selenium\n",
    "### MAKE SURE TO USE SLEEP SO YOU DON'T GET BLOCKED!!!\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(1.5) # force wait time (need to check documentation)\n",
    "driver.get(WRESTLING_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Following cells navigate from trackwrestling landing page to New York State events pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(\"displayMenu('subMenu-browse', 'left', 1, 'Browse')\")\n",
    "sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(\"displayMenu('subMenu-seasons', 'left', 2, 'Seasons')\")\n",
    "sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(\"displayMenu('subMenu-seasonOrganizeBy_1428400132', 'left', 3, '2019-20 High School Boys')\")\n",
    "sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(\"gotoBuildURLPage('seasons/LoadBalance.jsp?seasonId=1428400132&gbId=38&uname=&pword=')\")\n",
    "sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save cookies\n",
    "pickle.dump(driver.get_cookies(), open(\"cookies.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add cookies for page access\n",
    "cookies = pickle.load(open(\"cookies.pkl\", \"rb\"))\n",
    "for cookie in cookies:\n",
    "    driver.add_cookie(cookie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile regex for elements we want to find\n",
    "frameDataGrid_regex = re.compile(r'initDataGrid\\(50, true, \\\"(.*\\]\\])')\n",
    "initDataGrid_regex = re.compile(r'initDataGrid\\(1000, false, \\\"(.*\\]\\])')\n",
    "eventName_regex = re.compile(r'drawPageHeader\\(\\\"(.*)\\\"\\)')\n",
    "eventId_regex = re.compile(r'eventId=(.*)&')\n",
    "#teamId_regex = re.compile(r'teamId=(.*)')\n",
    "dualId_regex = re.compile(r'dualId=(.*)&')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scrapes info of each match for every event/dual in the last 250 entries to trackwrestling\n",
    "### This runs first for initial data, then we use daily scrape notebook to update\n",
    "\n",
    "# Look into adding more wait time when opening dual window\n",
    "# Play with length of sleeps, try to minimize\n",
    "# Current runtime: ~40 minutes\n",
    "\n",
    "match_Id = 0\n",
    "error_list = [] # list of dicts\n",
    "empty_record_list = [] # nested list of bools\n",
    "\n",
    "events_url = driver.current_url\n",
    "default_window = driver.window_handles[0]\n",
    "\n",
    "driver.switch_to.frame('PageFrame')\n",
    "\n",
    "# Collect all event names and dates for timestamping\n",
    "frame_html = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
    "frame_data = frameDataGrid_regex.findall(frame_html)\n",
    "\n",
    "# Remove control chars that make json.loads unhappy\n",
    "frame_data = frame_data[0]\n",
    "frame_data = re.sub(r'[\\t\\n\\r\\\\]','',frame_data)\n",
    "frame_data_list = json.loads(frame_data)\n",
    "\n",
    "eventNames = [data[5].strip() for data in frame_data_list] # events have names, duals are blank\n",
    "eventDates = [data[3] for data in frame_data_list] #string in YYYYMMDD format\n",
    "\n",
    "# Show all 250 entries for easier scraping\n",
    "max_value = driver.find_element_by_xpath(\"//input[@value='50']\") # searches based on default value\n",
    "max_value.send_keys(Keys.CONTROL,'a')\n",
    "max_value.send_keys('250')\n",
    "max_value.send_keys(Keys.ENTER)\n",
    "\n",
    "for i in range(0,250): # check all 250 events on page\n",
    "    \n",
    "    # openEvent either opens the match results in new window or asks user to pick a specific team\n",
    "    eventDate = eventDates[i]\n",
    "    driver.execute_script(\"openEvent({})\".format(i))\n",
    "    sleep(uniform(0.5,0.7))\n",
    "    \n",
    "    if len(driver.window_handles) == 1: # user prompted; need to loop through various teams\n",
    "        \n",
    "        frame_html = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
    "        frame_soup = BeautifulSoup(frame_html,'html.parser')\n",
    "        \n",
    "        all_links = [str(link.get('href')) for link in frame_soup.find_all('a')]\n",
    "        max_link_length = max(map(len,all_links)) # the links we want will always be the longest in the page\n",
    "        team_links = [link for link in all_links if len(link)==max_link_length]\n",
    "        sleep(uniform(0.5,0.7))\n",
    "        \n",
    "        empties = [] # 1 if link is empty, 0 if it has data\n",
    "        \n",
    "        for link in team_links: \n",
    "                        \n",
    "            driver.get(WRESTLING_URL+'/seasons/'+link)\n",
    "            sleep(uniform(0.5,0.7))\n",
    "            \n",
    "            match_html = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
    "            \n",
    "            eventName = eventName_regex.findall(match_html)\n",
    "                \n",
    "            if not eventName: # error page, save error info and back out\n",
    "                \n",
    "                bad_url = driver.current_url\n",
    "                temp_error_dict = {'URL':bad_url,'Event Number':i,'HTML':match_html} # can record more info as desired\n",
    "                error_list.append(temp_error_dict)\n",
    "                \n",
    "                driver.execute_script(\"window.history.go(-1)\")\n",
    "                sleep(uniform(0.5,0.7))\n",
    "                default_window = driver.window_handles[0]\n",
    "                sleep(uniform(0.5,0.7))\n",
    "                driver.switch_to.frame('PageFrame')\n",
    "                \n",
    "            else: # real page, all good\n",
    "                \n",
    "                eventName = eventName[0].strip()\n",
    "                #teamId = teamId_regex.findall(link)[0]\n",
    "                eventId = eventId_regex.findall(link)[0]\n",
    "                dualId = None\n",
    "            \n",
    "                match_data = initDataGrid_regex.findall(match_html)\n",
    "                \n",
    "                if not match_data: # empty record, save to check later\n",
    "                    \n",
    "                    empties.append(1)\n",
    "                \n",
    "                else: # data exists, scrape it\n",
    "                \n",
    "                    empties.append(0)\n",
    "                \n",
    "                    # remove control chars that make json.loads unhappy\n",
    "                    match_data = match_data[0]\n",
    "                    match_data = re.sub(r'[\\t\\n\\r\\\\]','',match_data)\n",
    "                    match_data_list = json.loads(match_data)\n",
    "\n",
    "                    # note order of appended items\n",
    "                    for each_match in match_data_list:\n",
    "\n",
    "                        each_match.append(eventName)\n",
    "                        each_match.append(eventId)\n",
    "                        each_match.append(dualId)\n",
    "                        each_match.append(eventDate)\n",
    "\n",
    "                    match_Id += 1\n",
    "\n",
    "                    json.dump(match_data_list,\n",
    "                             open(OUTPUT_FILE.format(match_Id),'w'))\n",
    "                    \n",
    "        # add empties to empty record list\n",
    "        empty_record_list.append(empties)\n",
    "        \n",
    "        # return to events page ready to open next event\n",
    "        driver.get(events_url)\n",
    "        sleep(uniform(0.5,0.7))\n",
    "        default_window = driver.window_handles[0]\n",
    "        sleep(uniform(0.5,0.7))\n",
    "        driver.switch_to.frame('PageFrame')\n",
    "        sleep(uniform(0.5,0.7))\n",
    "        \n",
    "        # Show all 250 entries for easier scraping\n",
    "        max_value = driver.find_element_by_xpath(\"//input[@value='50']\")\n",
    "        max_value.send_keys(Keys.CONTROL,'a')\n",
    "        max_value.send_keys('250')\n",
    "        max_value.send_keys(Keys.ENTER)\n",
    "\n",
    "    else: # match info opened in new window\n",
    "        \n",
    "        driver.switch_to.window(default_window)\n",
    "        driver.switch_to.default_content\n",
    "        \n",
    "        match_window = driver.window_handles[1] # remember new window\n",
    "        sleep(uniform(0.5,0.7))\n",
    "        \n",
    "        driver.switch_to.window(match_window) # switch to new window\n",
    "        sleep(uniform(0.5,0.7))\n",
    "            \n",
    "        match_html = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
    "        link = driver.current_url\n",
    "\n",
    "        eventName = eventName_regex.findall(match_html)\n",
    "        \n",
    "        if not eventName: # not all good, record error info\n",
    "            \n",
    "            temp_error_dict = {'URL':link,'Event Number':i,'HTML':match_html} # can record more info as desired\n",
    "            error_list.append(temp_error_dict)\n",
    "        \n",
    "        else: # real page, all good\n",
    "                \n",
    "            eventName = eventName[0].strip()\n",
    "            #teamId = teamId_regex.findall(link)[0]\n",
    "            dualId = dualId_regex.findall(link)[0]\n",
    "            eventId = None\n",
    "            \n",
    "            match_data = initDataGrid_regex.findall(match_html)\n",
    "            \n",
    "            if not match_data: # empty record, save to check later\n",
    "                \n",
    "                empty_record_list.append(1)\n",
    "            \n",
    "            else: # data exists, scrape it\n",
    "                \n",
    "                empty_record_list.append(0)\n",
    "                \n",
    "                # remove control chars that make json.loads unhappy\n",
    "                match_data = match_data[0]\n",
    "                match_data = re.sub(r'[\\t\\n\\r\\\\]','',match_data)\n",
    "                match_data_list = json.loads(match_data)\n",
    "\n",
    "                # note order of appended items\n",
    "                for each_match in match_data_list:\n",
    "\n",
    "                    each_match.append(eventName)\n",
    "                    each_match.append(eventId)\n",
    "                    each_match.append(dualId)\n",
    "                    each_match.append(eventDate)\n",
    "\n",
    "                match_Id += 1\n",
    "\n",
    "                json.dump(match_data_list,\n",
    "                            open(OUTPUT_FILE.format(match_Id),'w'))\n",
    "                \n",
    "        # closes new window and returns to events page to open next event\n",
    "        driver.close()\n",
    "        driver.switch_to.window(default_window)\n",
    "        sleep(uniform(0.5,0.7))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save number of individual JSON files for later use\n",
    "\n",
    "with open('./ScrapedMatchData/NumFiles.txt','w') as NumFile:\n",
    "    NumFile.write(str(match_Id))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save names and dates of scraped events for later use\n",
    "\n",
    "json.dump(eventNames,\n",
    "            open('./ScrapedMatchData/ScrapedEventNames.json','w'))\n",
    "\n",
    "json.dump(eventDates,\n",
    "            open('./ScrapedMatchData/ScrapedEventDates.json','w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save empty record info so we know to check them for updates later\n",
    "\n",
    "json.dump(empty_record_list,\n",
    "            open('./ScrapedMatchData/EmptyRecords.json','w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save error info to investigate later\n",
    "\n",
    "json.dump(error_list,\n",
    "            open('./ScrapedMatchData/ScrapingErrors.json','w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
