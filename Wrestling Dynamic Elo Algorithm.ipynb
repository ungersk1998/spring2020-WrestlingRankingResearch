{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from elote import EloCompetitor, LambdaArena\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in full matches and wrestlers dataframes\n",
    "full_matches = pd.read_csv('MATCHES.csv').drop(columns=\"Unnamed: 0\")\n",
    "full_wrestlers = pd.read_csv('WRESTLERS.csv').drop(columns=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_matches.shape # number of matches, recorded match variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_matches.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_wrestlers.shape # number of wrestlers, recorded wrestler variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_wrestlers.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop matches decided by useless wincons like forfeit or bye (ASSUMPTION: these wincons don't imply wrestler ability)\n",
    "bad_wins = ['Forfeit','Injury Default','Medical Forfeit','Bye','Disqualified','Default','No Contest']\n",
    "win_filter = [win not in bad_wins for win in full_matches[\"Victory Type (L)\"]]\n",
    "MATCHES = full_matches.loc[win_filter].drop_duplicates().reset_index(drop=True) # dedupe seems to work now\n",
    "MATCHES.sort_values(by='Event Date',ascending=True,inplace=True,ignore_index=True) # Make sure matches are sorted by date\n",
    "# Go back and check if dedupe is removing more than it should -- maybe not distinguishing multiple bouts on same day\n",
    "MATCHES.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy infoscrape function from Wrestling Tables notebook\n",
    "\n",
    "def infoscrape(fullname,df):\n",
    "    '''infoscrape receives full name of wrestler and matches dataframe\n",
    "    and collects wrestler info from dataset'''\n",
    "\n",
    "    # Initialize values of interest\n",
    "    weight_class = 0\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    matches = 0\n",
    "    school = ''\n",
    "    school_code = ''\n",
    "    first_name = ''\n",
    "    last_name = ''\n",
    "    \n",
    "    # Find observations corresponding to wrestler name\n",
    "    win_id = df['Winner Full Name'] == fullname\n",
    "    loss_id = df['Loser Full Name'] == fullname\n",
    "    winning_matches = df.loc[win_id,:]\n",
    "    losing_matches = df.loc[loss_id,:]\n",
    "    \n",
    "    # Split full name\n",
    "    first_name, last_name = fullname.split(' ',1)\n",
    "    \n",
    "    # Counting stats (should check if names show in correct columns for forfeits, byes, etc.)\n",
    "    wins = sum(win_id)\n",
    "    losses = sum(loss_id)\n",
    "    matches = wins+losses\n",
    "    \n",
    "    # Extract weight class, school, etc.\n",
    "    win_weight = winning_matches['Weight Class'].unique()\n",
    "    loss_weight = losing_matches['Weight Class'].unique()\n",
    "    \n",
    "    if win_weight.size > 0: # Avoiding 'if win_weight:' because it gives truth amibiguity warning\n",
    "        weight_class = int(win_weight[0])\n",
    "    else: # !!!Still need to add consideration for multiple weight classes!!!\n",
    "        weight_class = int(loss_weight[0])\n",
    "        \n",
    "    win_school = winning_matches['Winner School (L)'].unique()\n",
    "    win_school_code = winning_matches['Winner School (S)'].unique()\n",
    "    loss_school = losing_matches['Loser School (L)'].unique()\n",
    "    loss_school_code = losing_matches['Loser School (S)'].unique()\n",
    "    \n",
    "    if win_school.size > 0: # Avoiding 'if win_school:' because it gives truth amibiguity warning\n",
    "        school = win_school[0]\n",
    "        school_code = win_school_code[0]\n",
    "    else: \n",
    "        school = loss_school[0]\n",
    "        school_code = loss_school_code[0]\n",
    "        \n",
    "    # Return list of extracted data \n",
    "    return({'First Name':first_name,'Last Name':last_name,'Full Name':fullname,\n",
    "            'School Name':school,'School Code':school_code,\n",
    "            'Weight Class':weight_class,'Wins':wins,'Losses':losses,'Matches':matches})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remake wrestlers df\n",
    "# Note: union of winner/loser full names is set of all wrestlers in dataset\n",
    "wrestlers = set(MATCHES['Winner Full Name']) | set(MATCHES['Loser Full Name'])\n",
    "wrestlers = [x for x in wrestlers if x==x] # remove nan, convert to list\n",
    "wrestler_data = [infoscrape(wrestler,MATCHES) for wrestler in wrestlers]\n",
    "WRESTLERS = pd.DataFrame(wrestler_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRESTLERS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create train and test data split by date of wrestling events\n",
    "# Note: research paper trained on one weight class and tested on all the rest.\n",
    "# Why this is a big separate function: have to remake wrestlers dataframe from filtered matches dataframe\n",
    "\n",
    "def train_test_split(match_data, wrestler_data=None, split_method='date',\n",
    "                    earliest=None, latest=None, train_size=0.75):\n",
    "    '''train_test_split creates train and test data using given match data.\n",
    "    Can split by date range for train set or desired train data size (default is date).\n",
    "    Train_size is between 0 and 1. earliest/latest are integer dates in format YYYYMMDD.\n",
    "    Returns dict of match_train, match_test, wrestler_train, wrestler_test.'''\n",
    "    \n",
    "    event_dates = match_data[\"Event Date\"]\n",
    "    \n",
    "    # Default dates\n",
    "    if earliest is None:\n",
    "        earliest = min(event_dates)\n",
    "    if latest is None:\n",
    "        latest = max(event_dates)\n",
    "    \n",
    "    # Handle input exceptions        \n",
    "    if latest > max(event_dates):\n",
    "        raise ValueError('Invalid indexing: latest ({}) cannot be after most recent event ({})'\\\n",
    "                         .format(latest,max(event_dates)))\n",
    "    if earliest >= latest:\n",
    "        raise ValueError('Invalid indexing: earliest ({}) must be less than latest ({})'\\\n",
    "                         .format(earliest,latest))\n",
    "        \n",
    "    # Train-Test Split\n",
    "    \n",
    "    if split_method == 'size': # split by train_size\n",
    "        \n",
    "        indices = match_data.index.values\n",
    "        n = len(indices)\n",
    "        train_end = int(n*train_size)\n",
    "        train_id = range(0,train_end)\n",
    "        test_id = range(train_end,n)\n",
    "        match_train = match_data.iloc[train_id,:]\n",
    "        match_test = match_data.iloc[test_id,:]\n",
    "        \n",
    "    if split_method == 'date': # split by date range\n",
    "        \n",
    "        date_range = range(earliest,latest+1)\n",
    "        train_bool = [date in date_range for date in event_dates]\n",
    "        test_bool = [not index for index in train_bool]\n",
    "        match_train = match_data.loc[train_bool]\n",
    "        match_test = match_data.loc[test_bool]\n",
    "        \n",
    "        \n",
    "    # Name wrestlers to train or test sets\n",
    "    wrestler_names_train = set(match_train['Winner Full Name']) | set(match_train['Loser Full Name'])\n",
    "    wrestler_names_train = [x for x in wrestler_names_train if x==x] # remove nan, convert to list\n",
    "    \n",
    "    # Not sure if making wrestler test set like this makes total sense but I'll do it for now\n",
    "    # Maybe because of cumulative stats, wrestler test set is always up-to-date full wrestler data?\n",
    "    wrestler_names_test = set(match_test['Winner Full Name']) | set(match_test['Loser Full Name'])\n",
    "    wrestler_names_test = [x for x in wrestler_names_test if x==x] # remove nan, convert to list\n",
    "\n",
    "    # Call infoscrape to construct wrestler dataframes\n",
    "    wrestler_train = [infoscrape(wrestler,match_train) for wrestler in wrestler_names_train]\n",
    "    wrestler_train = pd.DataFrame(wrestler_train)\n",
    "    wrestler_test = [infoscrape(wrestler,match_test) for wrestler in wrestler_names_test]\n",
    "    wrestler_test = pd.DataFrame(wrestler_test)\n",
    "    \n",
    "    # Store train/test splits in dict\n",
    "    train_test_dict = {\"match_train\":match_train,\"match_test\":match_test,\n",
    "                      \"wrestler_train\":wrestler_train,\"wrestler_test\":wrestler_test}\n",
    "    \n",
    "    return(train_test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest(arr, K): \n",
    "    '''returns the item in arr closest to the value K'''\n",
    "    idx = (np.abs(arr - K)).argmin() \n",
    "    return(arr[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elo_matchups(match_train):\n",
    "    \n",
    "    train_matchups = list()\n",
    "    \n",
    "    for idx, row in match_train.iterrows():\n",
    "        w1 = row['Winner Full Name']\n",
    "        w2 = row['Loser Full Name']\n",
    "        \n",
    "        # nan entry -> just have wrestler go against himself for now (should result in no winner)\n",
    "        # no option for both nan, but that is a datapoint I don't even want\n",
    "        if w1!=w1:\n",
    "            w1 = w2\n",
    "        elif w2!=w2:\n",
    "            w2 = w1\n",
    "            \n",
    "        train_matchups.append((\n",
    "            w1,w2\n",
    "        ))\n",
    "        \n",
    "    return(train_matchups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elote_func(a, b): # Currenly trivial since winner is already known to be first entry, a\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_recorder(matchup,wrestler_train,school_competitor_dict,initial=1000):\n",
    "    '''Records a given matchup, implementing cold start ratings if needed, and updates school_competitor_dict.'''\n",
    "    \n",
    "    # Extract dict keys\n",
    "    winner = matchup[0]\n",
    "    loser = matchup[1]\n",
    "    observed_schools = school_competitor_dict.keys()\n",
    "    \n",
    "    # Schools\n",
    "    winner_bool = wrestler_train[\"Full Name\"]==winner\n",
    "    winner_school = wrestler_train.loc[winner_bool][\"School Name\"].values[0]\n",
    "    loser_bool = wrestler_train[\"Full Name\"]==loser\n",
    "    loser_school = wrestler_train.loc[loser_bool][\"School Name\"].values[0]\n",
    "    \n",
    "    # Cold-start cases: Winner\n",
    "    if winner_school not in observed_schools: # Wrestler receives static initial rating, begins their school's data\n",
    "        school_competitor_dict[winner_school] = {}\n",
    "        school_competitor_dict[winner_school][winner] = EloCompetitor(initial_rating=initial)\n",
    "        \n",
    "    else: # Wrestler has school data\n",
    "        school_wrestlers = school_competitor_dict[winner_school].keys()\n",
    "        \n",
    "        if winner not in school_wrestlers: # Wrestler receives dynamic initial rating, mean of teammates ratings\n",
    "            team_ratings = [competitor.rating for competitor in list(school_competitor_dict[winner_school].values())]\n",
    "            winner_rating = np.mean(team_ratings)\n",
    "            school_competitor_dict[winner_school][winner] = EloCompetitor(initial_rating=winner_rating)\n",
    "            \n",
    "    # Cold-start cases: Loser\n",
    "    if loser_school not in observed_schools: # Wrestler receives static initial rating, begins their school's data\n",
    "        school_competitor_dict[loser_school] = {}\n",
    "        school_competitor_dict[loser_school][loser] = EloCompetitor(initial_rating=initial)\n",
    "        \n",
    "    else: # Wrestler has school data\n",
    "        school_wrestlers = school_competitor_dict[loser_school].keys()\n",
    "        \n",
    "        if loser not in school_wrestlers: # Wrestler receives dynamic initial rating, mean of teammates ratings\n",
    "            team_ratings = [competitor.rating for competitor in list(school_competitor_dict[loser_school].values())]\n",
    "            loser_rating = np.mean(team_ratings)\n",
    "            school_competitor_dict[loser_school][loser] = EloCompetitor(initial_rating=loser_rating)\n",
    "            \n",
    "    # Record match result\n",
    "    school_competitor_dict[winner_school][winner].beat(school_competitor_dict[loser_school][loser])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elo support provided via elote package by Will McGinnis @ wdm0006 on GitHub\n",
    "# Remember to make this input a generic alg_args dict when that gets changed\n",
    "\n",
    "def elo_rank(match_train,wrestler_train,initial=1000,K=225):\n",
    "    '''elo_rank takes in match data along with elo tuning params and returns an ordered ranking of wrestlers\n",
    "    along with the saved state of the elo arena'''\n",
    "    \n",
    "    \n",
    "    # Form train matchups. For this, winner will always be first listed in each matchup.\n",
    "    train_matchups = elo_matchups(match_train)\n",
    "    \n",
    "    # Create arena and process train matches\n",
    "    arena = LambdaArena(elote_func)\n",
    "    arena.set_competitor_class_var('_k_factor',K)\n",
    "    school_competitor_dict = {} # dict of dicts of dicts (outer_dict -> school -> wrestler -> EloCompetitor Object)\n",
    "    \n",
    "    for matchup in train_matchups:\n",
    "        match_recorder(matchup,wrestler_train,school_competitor_dict,initial)\n",
    "        \n",
    "    lb = [\n",
    "        {'competitor':item[0],'rating':item[1].rating}\n",
    "        for d in list(school_competitor_dict.values())\n",
    "        for item in d.items()\n",
    "    ]\n",
    "\n",
    "    lb = sorted(lb, reverse=True, key=lambda x: x.get('rating'))\n",
    "    \n",
    "    # Return rankings and arena, post-train matches\n",
    "    saved_state = arena.export_state()\n",
    "    return({\"Rankings\":lb,\n",
    "            \"Saved Arena\":saved_state})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some potentially useful elo ranking dictionaries\n",
    "\n",
    "def elo_rank_dicts(raw_rankings):\n",
    "    \n",
    "    wrestlers_with_rankings = {} # keys are wrestlers, values are (rankings,ratings)\n",
    "    rankings_with_wrestlers = {} # keys are rankings, (wrestlers,ratings) are values\n",
    "\n",
    "    for index, comp_dict in enumerate(raw_rankings, start=1):\n",
    "        rank = index\n",
    "        wrestler,rating = comp_dict.values()\n",
    "        rating = round(rating,3)\n",
    "        wrestlers_with_rankings[wrestler] = (rank,rating)\n",
    "        rankings_with_wrestlers[rank] = (wrestler,rating)\n",
    "        \n",
    "    return({\"Ranked Wrestlers\":wrestlers_with_rankings,\n",
    "            \"Named Rankings\":rankings_with_wrestlers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elo_pred(test_matchups,wrestler_train,wrestler_test,match_train,match_test): # alg_args please\n",
    "    '''elo_pred predicts the winner of a bout to be the wrestler with the higher elo rating. Performs \n",
    "    predictions on all test matchups at once.'''\n",
    "    \n",
    "    \n",
    "    # Raw rankings\n",
    "    elo_rank_output = elo_rank(match_train,wrestler_train)\n",
    "    elo_rankings = elo_rank_output[\"Rankings\"]\n",
    "    \n",
    "    # Nicer dicts\n",
    "    nice_dicts = elo_rank_dicts(elo_rankings)\n",
    "    ranked_wrestlers = nice_dicts[\"Ranked Wrestlers\"]\n",
    "    named_rankings = nice_dicts[\"Named Rankings\"]\n",
    "    \n",
    "    ## initial_state arg for arena call doesn't seem to work, commented out dependent code and implemented temp solution\n",
    "    \n",
    "    # Extract,implement arena info \n",
    "    #saved_state = elo_rank_output[\"Saved Arena\"]\n",
    "    #arena = LambdaArena(elote_func,initial_state=saved_state)\n",
    "    \n",
    "    # Simulate wrestlers' bout\n",
    "    #winprob1 = arena.expected_score(wrestler1,wrestler2) # probability of wrestler1 beating wrestler2\n",
    "    # Do we want a minimal difference to declare a decision?\n",
    "    # Can reformat this into a more general confidence measure\n",
    "    #prob_diff = abs(2*(winprob1 - 0.5))\n",
    "\n",
    "    \n",
    "    #if winprob1 > 0.5:\n",
    "    #    return({\"Winner\":wrestler1,\"Confidence\":prob_diff})\n",
    "    #elif winprob1 == 0.5:\n",
    "    #    return({\"Winner\":None,\"Confidence\":prob_diff}) # TODO: Track these in validation\n",
    "    #else:\n",
    "    #    return({\"Winner\":wrestler2,\"Confidence\":prob_diff})\n",
    "    \n",
    "    \n",
    "    # Temp solution\n",
    "    pred_outputs = [] # list of dicts, each with two keys, 'Winner' and 'Confidence'\n",
    "    default_rating = np.median([pair[1] for pair in ranked_wrestlers.values()]) # use median rating for default guess\n",
    "    \n",
    "    for matchup in test_matchups:\n",
    "        \n",
    "        output_dict = {}\n",
    "        \n",
    "        wrestler1 = matchup[0]\n",
    "        wrestler2 = matchup[1]\n",
    "    \n",
    "        if wrestler1 not in ranked_wrestlers.keys():\n",
    "            rating1 = default_rating\n",
    "        else:\n",
    "            rating1 = ranked_wrestlers[wrestler1][1]\n",
    "\n",
    "        if wrestler2 not in ranked_wrestlers.keys():\n",
    "            rating2 = default_rating\n",
    "        else:\n",
    "            rating2 = ranked_wrestlers[wrestler2][1]\n",
    "            \n",
    "        rating_diff = abs(rating1-rating2) # make this a comparable confidence format; see Win Percentage Pred function\n",
    "        output_dict['Confidence'] = rating_diff\n",
    "               \n",
    "        \n",
    "        if rating1 > rating2: # Wrestler 1 wins\n",
    "            output_dict['Winner'] = wrestler1\n",
    "        elif rating1==rating2:\n",
    "            output_dict['Winner'] = None\n",
    "        else: # Wrestler 2 wins\n",
    "            output_dict['Winner'] = wrestler2\n",
    "            \n",
    "        pred_outputs.append(output_dict)\n",
    "            \n",
    "    return(pred_outputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give pred algorithms attributes so that the alogorithm tester can test them differently (for efficiency)\n",
    "# Can also probably just work test_method into algorithm args\n",
    "setattr(elo_pred,'test_method','batch') # batch because elo_pred can rank everyone and predict in one go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchmaker(match_test):\n",
    "    '''matchmaker takes in match test data and returns a list of \n",
    "    the associated wrestler matchup pairs'''\n",
    "    \n",
    "    \n",
    "    test_matchups = []\n",
    "    \n",
    "    for i in range(0,match_test.shape[0]):\n",
    "        match = match_test.iloc[i]\n",
    "        w1 = match[\"Winner Full Name\"]\n",
    "        w2 = match[\"Loser Full Name\"]\n",
    "        \n",
    "        # nan entry -> just have wrestler go against himself for now (should result in no winner)\n",
    "        # no option for both nan, but that is a datapoint I don't even want\n",
    "        if w1!=w1:\n",
    "            w1 = w2\n",
    "        elif w2!=w2:\n",
    "            w2 = w1\n",
    "        \n",
    "        test_matchups.append((w1,w2)) # winner is first entry in each matchup tuple for testing\n",
    "        \n",
    "    return(test_matchups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_algorithm(algorithm,match_train,match_test,wrestler_train,wrestler_test):\n",
    "    '''test_algorithm implements a given algorithm using given wrestler and match \n",
    "    train/test data and returns prediction accuracy'''\n",
    "    \n",
    "    # Extract matchups from test matches\n",
    "    test_matchups = matchmaker(match_test)\n",
    "    \n",
    "    # Extract test_method attribute from algorithm\n",
    "    test_method = algorithm.test_method\n",
    "\n",
    "    # True and predicted winners\n",
    "    true_winners = match_test[\"Winner Full Name\"]\n",
    "    \n",
    "    if test_method == 'batch': # Elo test\n",
    "        pred_output = algorithm(test_matchups,wrestler_train,wrestler_test,match_train,match_test)\n",
    "    else: # WP test\n",
    "        pred_output = [algorithm(W1,W2,wrestler_train,wrestler_test,match_train,match_test) for W1,W2 in test_matchups]\n",
    "        \n",
    "    pred_winners = [output[\"Winner\"] for output in pred_output]\n",
    "    pred_confidences = [output[\"Confidence\"] for output in pred_output] # Needs normalization for comparison b/t algs\n",
    "    \n",
    "    # Calculate prediction accuracy, save incorrect pred info\n",
    "    no_decision_bool = [pred is None for pred in pred_winners]\n",
    "    no_decisions = sum(no_decision_bool)\n",
    "    no_decision_preds = match_test.loc[no_decision_bool,:]\n",
    "    pred_results = true_winners == pred_winners\n",
    "    incorrect_preds = match_test.loc[true_winners != pred_winners,:]\n",
    "    n = len(pred_results)\n",
    "    correct = sum(pred_results)\n",
    "    incorrect = n - correct\n",
    "    pred_accuracy = pred_results.mean()\n",
    "    \n",
    "    return({\"Accuracy\":pred_accuracy,\"NumCorrect\":correct,\"NumIncorrect\":incorrect,\"NumUndecided\":no_decisions,\"N\":n,\n",
    "           \"WrongPreds\":incorrect_preds,\"UndecidedPreds\":no_decision_preds,\"PredConfidences\":pred_confidences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test algorithms; various training sizes. Takes 10-15 minutes cycling through each train_size\n",
    "train_sizes = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "elo_results = {}\n",
    "\n",
    "for size in train_sizes:\n",
    "    \n",
    "    # latest = int(MATCHES['Event Date'].quantile(q=size))\n",
    "    train_test_dict = train_test_split(MATCHES,split_method='size',train_size=size)\n",
    "    match_train = train_test_dict[\"match_train\"]\n",
    "    wrestler_train = train_test_dict[\"wrestler_train\"]\n",
    "    match_test = train_test_dict[\"match_test\"]\n",
    "    wrestler_test = train_test_dict[\"wrestler_test\"]\n",
    "    \n",
    "    elo_pred_results = test_algorithm(elo_pred,match_train,match_test,wrestler_train,wrestler_test)\n",
    "    \n",
    "    elo_results[size] = elo_pred_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract accuracy values\n",
    "elo_accuracy_dict = {}\n",
    "\n",
    "for size in train_sizes:\n",
    "    elo_accuracy_dict[size] = elo_results[size]['Accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(elo_accuracy_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy results: Dynamic Elo\n",
    "plt.bar(range(len(elo_accuracy_dict)), list(elo_accuracy_dict.values()), align='center')\n",
    "plt.xticks(range(len(elo_accuracy_dict)), list(elo_accuracy_dict.keys()))\n",
    "plt.title('Prediction Accuracy')\n",
    "plt.xlabel('Train Set Size')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Save fig\n",
    "plt.savefig('./Plots/Elo/dynamic_pred_accuracy.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Observation: Elo performs worse earlier, better later\n",
    "# Grain of salt: we don't even have the full season data, just late season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narrow diagnostics to best (0.9) train size results for now\n",
    "# Investigate erroneous predictions\n",
    "elo_pred_results = elo_results[0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elo_pred_results['NumUndecided']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elo_mistakes = elo_pred_results['WrongPreds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dynamic Elo Mistakes:\",elo_mistakes.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elo_mistakes.groupby('Event Name').size().reset_index(name='counts') # Mistakes by event name, Elo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show distribution of weight classes among wrestlers in incorrect pred cases\n",
    "# Misleading: should scale by num wrestler/matches in weight class\n",
    "# Note: make this a bar chart instead if possible\n",
    "elo_pred_results['WrongPreds'].hist(column=\"Weight Class\")\n",
    "plt.xlabel(\"Weight Class\")\n",
    "plt.ylabel(\"Number of Incorrect Predictions\")\n",
    "plt.title(\"Incorrect Elo Preds by Weight Class\")\n",
    "\n",
    "# Save fig\n",
    "plt.savefig('./Plots/Elo/dynamic_incorrect_preds_weight_class.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show distribution of weight classes among wrestlers in incorrect pred cases\n",
    "# Misleading: should scale by num wrestler/matches in weight class\n",
    "# Note: make this a bar chart instead if possible\n",
    "elo_pred_results['WrongPreds'].hist(column=\"Event Date\",bins=25)\n",
    "plt.xlabel(\"Event Date\")\n",
    "plt.ylabel(\"Number of Incorrect Predictions\")\n",
    "plt.title(\"Incorrect Dynamic Elo Preds by Event Date\")\n",
    "# fix x axis scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show distribution of number of matches among wrestlers\n",
    "WRESTLERS.hist(column=\"Matches\")\n",
    "plt.xlabel(\"Number of Matches\")\n",
    "plt.ylabel(\"Number of Wrestlers\")\n",
    "plt.title(\"Distribution of Wrestlers by Number of Matches\")\n",
    "\n",
    "# Save fig\n",
    "plt.savefig('./Plots/EDA/wrestler_match_dist.png')\n",
    "\n",
    "# Vast majority of wrestlers have less than 15 matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show distribution of matches by weight class\n",
    "wrestlers_by_weight = WRESTLERS.groupby('Weight Class')\n",
    "wrestlers_by_weight.sum().plot.bar(y=\"Matches\")\n",
    "plt.title(\"Distribution of Matches by Weight Class\")\n",
    "\n",
    "# Save fig\n",
    "plt.savefig('./Plots/EDA/matches_by_weight_class.png')\n",
    "\n",
    "# Fairly balanced weight classes; flattened bell curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check count of matches by victory type \n",
    "# There are nan entries for Victory Type (L)\n",
    "#matches_by_wintype = MATCHES.groupby('Victory Type (L)')\n",
    "#matches_by_wintype.describe()['Match ID'].plot.bar(y='count')\n",
    "#plt.title(\"Distribution of Matches by Victory Type\")\n",
    "\n",
    "# Save fig\n",
    "#plt.savefig('./Plots/matches_by_win_type.png')\n",
    "\n",
    "# Practically all victory types are fall or decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
